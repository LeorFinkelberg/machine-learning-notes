Детали можно найти на странице документации [Model Serving Runtimes](https://kserve.github.io/website/latest/modelserving/v1beta1/serving_runtime/).

KServe предоставляет простой Kubernetes CRD (Custom Runtime Definition), позволяющий разворачивать одну или несколько обученных моделей в средах выполнения, таких как TFServing, TorchServe, Triton Inference Server. Кроме того ModelServer -- это среда выполнения Python-моделей, реализованная в KServe с поддержкой протокола V1. MLServer реализует поддержку протокола V2 как для REST, так и для gRPC.

После того как модель была развернута с помощью сервиса логического вывода (InferenceServer), пользователь получает следующие бессерверные преимущества, предоставляемые KServe:
- Масштабирование до нуля и обратно
- Автомасштабирование с поддержкой CPU/GPU
- Оптимизированный контейнер
- Пакетирование (Batching)
- Логирование запросов / ответов
- Управление трафиком
- Безопасность с AuthN/AuthZ
- Распределенная трассировка
- Управление Ingress/Engress
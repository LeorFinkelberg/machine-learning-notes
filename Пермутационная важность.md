Подробности можно найти на странице Sklearn https://scikit-learn.org/stable/modules/permutation_importance.html.

ВАЖНО! Модель обучаем на обучающем поднаборе данных, а пермутационную важность вычисляем на валидационном поднаборе.

Пример
```python
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge

diabetes = load_diabetes()

X_train, X_val, y_train, y_val = train_test_split(
    diabetes.data, diabetes.target, random_state=0)

model = Ridge(alpha=1e-2).fit(X_train, y_train)
model.score(X_val, y_val)
```

```python
from sklearn.inspection import permutation_importance
r = permutation_importance(
		model, X_val, y_val,
        n_repeats=30,
        random_state=0
)

for i in r.importances_mean.argsort()[::-1]:
    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
        print(f"{diabetes.feature_names[i]:<8}"
              f"{r.importances_mean[i]:.3f}"
              f" +/- {r.importances_std[i]:.3f}")
```

Подробности можно найти в переписке Permutation Feature Importance on Categorical Features Dataset https://github.com/dotnet/machinelearning/issues/2597.

В случае, когда матрица признакового описания объекта содержит закодированные категориальные признаки, их пермутационную важность можно вычислять как обычно, то есть, рассматривая каждый столбец закодированного (например, с помощью OHE) признака по отдельности, но в итоге _суммируя полученные оценки важности_. Таким образом, каждый вещественный признак получит свою оценку и каждый столбец закодированного категориального признака получит свою оценку важности, но эти оценки нужно будет просуммировать, чтобы найти важность категориального признака.
```
Hi [@Vijay27anand](https://github.com/Vijay27anand) ! This is a very good question. `PermutationFeatureImportance` operates on feature vectors, not on raw columnar datasets, so it requires a bit of hands-on work to look at categorical features.

To get a sense for feature importance of categorical features, you'll need to look at the parts of the feature vector that encode for those categorical features. For example, if you do a simple one-hot encoding, then a categorical feature with four categories will create four slots in the feature vector, e.g. `1 0 0 0`, `0 1 0 0`, `0 0 1 0`, `0 0 0 1`. Hashing will do a very similar thing [1]. If you can work through your data and identify which slots of the feature vector your categorical sits in, then you can get your feature importance by _summing_ the importance of those slots (because the importance of your categorical is the combination of the importance of all those slots).

[1] In ML.NET, there is an option to "save" the input => output mapping which will allow you to work backwards from slot to categorical value
```
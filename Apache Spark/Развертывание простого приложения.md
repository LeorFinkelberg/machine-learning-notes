_Непрерывная интеграция_ (Continuous Integration -- CI) позволяет защитить непрерывно продолжающуюся работу одного разработчика от разрушения копией другой разработчика. Самые последние разработки этой концепции вводят в использование серверы сборки, которые автоматически выполняют модульные тесты с регулярными интервалами или даже после каждой передачи кода (commit) и сообщают результаты тестирования разработчикам.

_Непрерывная доставка_ (Continuous Delivery -- CD) позволяет разработчикам создавать программное обеспечение в коротких циклах, обеспечивая требуемую надежность программного обеспечения, вводимого в эксплуатацию в любое время. Методика непрерывной доставки ориентирована на сборку, тестирование и выпуск (ввод в эксплуатацию) программного обеспечения с значительно увеличенной скоростью и частотой.

Непрерывную доставку (Continuous Delivery) иногда путают с _непрерывным развертыванием_ (Continuous Deployment). При неправильном равзертывании любое изменение в производстве, которое успешно проходит последовательность тестов, автоматически развертывается в эксплуатационной среде. В отличие от этого при непрерывной доставке программное обеспечение требует надежной версии-релиза в любое время, но решение по вводу в эксплуатацию принимает человек обычно на основе бизнес-факторов.

Наиболее вероятный порядок сетевых вызовов:
1. Приложение устанавливает соединение с ведущим узлом
2. Устанавливается связь между рабочими узлами и ведущим узлом. Рабочие узлы инициализируют соединение, но данные передаются от ведущего узла на рабочие узлы.
3. Исполнителю необходима возможность обращения к драйверу, это означает, что драйвер не должен быть защищен сетевым экраном (firewall). Если исполнители лишены возможности обмена информацией с драйвером, то они не смогут передать данные обратно в приложение.  

Приложение Spark работают как независимые процессы в кластере. Объект `SparkSession` в _приложении_ (приложение также называют _программой-драйвером_) координирует рабочие процессы. Для каждого конкретного приложения создается собственный, единственный в своем роде объект `SparkSession` независимо от того, работаете ли вы в локальном режиме или в кластере с 10 000 узлов.

Объект `SparkSession` создается при создании конкретного сеанса
```python
spark = SparkSession.builder.appName("An app").master("local[*]").getOrCreate()
```

Вы также получаете контекст `SparkContext` как часть созданного сеанса. Контекст был единственным способом работы со Spark до версии v2. Сейчас по большей части нет необходимости взаимодействовать со `SparkContext`, но если потребуется (для доступа к информации об инфраструктуре, для создания аккумуляторов и пр.), то можно получить доступ так
```python
spark = SparkSession.builder.appName("An app").master("local[*]").getOrCreate()
context = spark.sparkContext

print(context.version)  # '3.5.1'
```

_Исполнители_ -- это процессы виртуальной машины Java (JVM), которые выполняют вычисления и сохраняют данные для приложения. Далее диспетчер кластера передает код приложения исполнителям. Вам не нужно заботиться о развертывании приложения на каждом узле. Наконец, сеанс `SparkSession` передает задачи (tasks) исполнителям для выполнения.

Вы всегда должны быть уверены в том, что исполнитель может обмениваться информацией с драйвером. Обмен информацией с драйвером означает, что исполнитель и драйвер не должны быть разделены защитным  сетевым экраном (firewall), не должны находиться в разных сетях. Программ-драйвер непременно должна находится в режиме прослушивания и принимать входящие соединения от своих исполнителей на протяжении всего жизненного цикла.

Поскольку _драйвер_ планирует задачи в кластере, он должен функционировать _физически близко к рабочим узлам_, предпочтительнее всего, _в одной и той же локальной сети_. Процессы весьма интенсивно используют сеть, поэтому при расположении физических сетевых узлов как можно ближе друг у другу сокращаются задержки в сети. Если вы планируете работать со Spark в _облачной среде_, то нужно договориться со своим облачным провайдером о выделении для вас компьютеров, расположенных физически близко друг к другу [[Литература#^61ef72]]<c. 173>.

Подробнее про режим кластера (Cluster Mode) можно почитать на странице документации [Cluster Mode Overview](https://spark.apache.org/docs/latest/cluster-overview.html).

### Замечания по установке Spark

1. Скачать tar-архив https://www.apache.org/dyn/closer.lua/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz со страницы проекта Apache Spark.
2. Распаковать
```bash
$ cd 
$ mkdir spark
$ cd spark
$ sudo wget ...  # download *.tgz
$ tar xzvf spark-3.5.1-bin-hadoop3.tgz
```
3. Запускаем ведущий узел
```bash
$ cd spark-3.5.1-bin-hadoop3/sbin
$ ./start-master  # запуск ведущего узла
```

Для того чтобы узнать URL ведущего узла можно зайти на http://localhost:8080. В моем случае ведущий узел получил URL `spark://leor-manjaro:7077`, где `leor-manjaro` это имя хоста, которое можно получить так
```bash
$ hostname  # leor-manjaro
$ echo $HOST  # leor-manjaro 
```

Также информацию по запуску ведущего узла можно найти в `spark-3.5.1-bin-hadoop3/logs/spark-leorfinkelberg-org.apache.spark.deploy.master.Master-1-leor-manjaro.out`
```bash
...
24/05/09 15:38:17 INFO Master: Starting Spark master at spark://leor-manjaro:7077
...
24/05/09 15:38:17 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://192.168.1.160:8080
...
```

Теперь, полученный URL ведущего узла, можно использовать для подключения рабочих узлов как
```bash
$ ./start-worker.sh spark://leor-manjaro:7077
```

После подключения рабочего узла можно обновить страницу http://localhost:8080 и проверить конфигурацию узла.

Приложение для вычисления числа $\pi$ 
```python
from pyspark.sql import SparkSession, functions as F

def compute_pi(...) -> float:
    ...
    return pi

if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("Compute Pi") \
        .master("spark://leor-manjaro:7077") \  # NB!!!
        .getOrCreate()
    compute_pi()
    spark.stop()
```

Отправить приложение на ведущий узел можно так
```bash
# Параметр --master не указывается, так как он получил значение при создании объекта Spark-сессии
$ spark-submit compute_pi.py
```

Или можно не вызывать цепочечный метод `.master("spark://...")` при создании объекта Spark-сессии, а передать URL ведущего узла параметру `--master` утилиты `spark-submit`
```bash
$ spark-submit --master spark://leor-manjaro:7077 compute_pi.py
```
Чтобы остановить рабочий и ведущий узлы, выполняем
```bash
$ ./stop-worker.sh
$ ./stop-master.sh
```
### Настройка среды кластера

Необходимо установить Spark _на каждом узле_. Приложение устанавливать на каждом узле нет необходимости.

На ведущем узле из-под каталога `/opt/apache-spark/sbin` следует выполнить команду
```bash
$ ./start-master.sh
```

Ведущий узел не выполняет большого объема работы, но ему постоянно требуются рабочие узлы. Для запуска первого _рабочего узла_ выполняется следующая команда
```bash
$ ./start-slave.sh spark://un:7077  # указываем URL ведущего узла
```

Каждый узел должен иметь возможность обмениваться информацией со всеми другими узлами в обоих направлениях. Порты не будут открываться в Интернет, только внутри локальной сети. Проверку можно выполнить с помощью утилит `ping`, `telnet` (`telnet <host> <port>`).

Первым всегда запускается ведущий узел, затем рабочие узлы.

Чтобы выполнить приложение в кластере, сначала нужно передать это приложение на _ведущий узел_
```bash
$ ./spark-submit \
    --class net.jgp.books.spark.ch05.lab210.piComputeCluster...
    --master "spark://un:7077"
	<path to>/spark-chapter05-1.0.0-SNAPSHOT.JAR
```

А Spark делает JAR-файл доступным для загрузки всеми рабочими узлами.

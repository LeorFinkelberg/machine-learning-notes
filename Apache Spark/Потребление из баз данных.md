При работе с БД, которая не поддерживается Spark, требуется специализированный диалект (dialect). Диалект -- это способ сообщить Spark о том, как нужно обмениваться информацией с конкретной БД. В дистрибутивный комплект Spark входит несколько диалектов, и в большинстве случаев даже не потребуется задумываться об их применении. Тем не менее существуют ситуации, в которых вы должны уметь создавать _специализированный диалект_.

Spark напрямую устанавливает соединение с реляционными БД, используя драйверы Java Database Connectivity (JDBC). _Диалект_  определяет поведение Spark при обмене информацией с БД.

### Фильтрация с использованием ключевого слова WHERE

Иногда нет необходимости копировать все данные из таблицы в кадр данных. Если известно, что некоторые данные не будут использоваться, то нет смысла копировать неиспользуемые строки, потому что _передача данных в сети_ -- ==дорогостоящая операция==.

Таким образом при обращении к БД можно запросить не всю таблицу целиком, а какой-то ее _фрагмент_ (параметр `query` в `.options()`).
```python
# run.py

from pathlib2 import Path
from pyspark.sql import SparkSession

_HOME = Path().home()
_POSTGRES_JDBC_PATH = "Documents/postgres-jdbc/postgresql-42.7.3.jar"
POSTGRES_JDBC_ABSPATH = str(_HOME.joinpath(_POSTGRES_JDBC_PATH).absolute())
DB_URL = "jdbc:postgresql://localhost:5432/spark_labs"

# Здесь указываем SQL-запрос, чтобы извлечь не всю таблицу, а лишь несколько строк
options = {
	"url": DB_URL,
	"driver": "org.postgresql.Driver",
	"user": "postgres",
	"query": "SELECT * FROM ch02 WHERE lastname ~* '^fin.*$'",  # <= NB
}

# Для того чтобы можно было работать с PostgreSQL, обязательно следует задать `spark.jars`
def main():
	spark = SparkSession \
	    .builder \
	    .appName("Work with PostgreSQL") \
	    .master("local[*]") \
	    .config(  # <= NB!!!
	        "spark.jars",
			POSTGRES_JDBC_ABSPATH 
	    ) \
	    .getOrCreate()
	
	df = spark.read.format("jdbc").options(**options).load()


if __name__ == "__main__":
    main()
    
```

При запуске приложения обязательно нужно указать путь до скаченного JAR-файла JDBC
```bash
$ export SPARK_CLASSPATH=~/Documents/postgres-jdbc/postgresql-42.7.3.jar
$ spark-submit --driver-class-path ${SPARK_CLASSPATH} run.py
```

Если же все-таки требуется извлечь все записи таблицы, то в `.options()` необходимо указать ключ `dbtable` и передать ему имя интересующей таблицы.
```python
from pathlib2 import Path
from pyspark.sql import SparkSession

_HOME = Path().home()
_POSTGRES_JDBC_PATH = "Documents/postgres-jdbc/postgresql-42.7.3.jar"
POSTGRES_JDBC_ABSPATH = str(_HOME.joinpath(_POSTGRES_JDBC_PATH).absolute())
DB_URL = "jdbc:postgresql://localhost:5432/spark_labs"

# Здесь указываем SQL-запрос, чтобы извлечь не всю таблицу, а лишь несколько строк
options = {
	"url": DB_URL,
	"driver": "org.postgresql.Driver",
	"user": "postgres",
    "dbtable": "ch02",  # <= NB
}

# Для того чтобы можно было работать с PostgreSQL, обязательно следует задать `spark.jars`
def main():
	spark = SparkSession \
	    .builder \
	    .appName("Work with PostgreSQL") \
	    .master("local[*]") \
	    .config(  # <= NB
	        "spark.jars",
			POSTGRES_JDBC_ABSPATH 
	    ) \
	    .getOrCreate()
	
	df = spark.read.format("jdbc").options(**options).load()


if __name__ == "__main__":
    main()
    
```
Полезные замечания можно найти в заметке [PySpark Read and Write MySQL Database Table]().

При работе с БД, которая не поддерживается Spark, требуется специализированный диалект (dialect). Диалект -- это способ сообщить Spark о том, как нужно обмениваться информацией с конкретной БД. В дистрибутивный комплект Spark входит несколько диалектов, и в большинстве случаев даже не потребуется задумываться об их применении. Тем не менее существуют ситуации, в которых вы должны уметь создавать _специализированный диалект_.

Spark напрямую устанавливает соединение с реляционными БД, используя драйверы Java Database Connectivity (JDBC). _Диалект_  определяет поведение Spark при обмене информацией с БД.

### Фильтрация с использованием ключевого слова WHERE

Иногда нет необходимости копировать все данные из таблицы в кадр данных. Если известно, что некоторые данные не будут использоваться, то нет смысла копировать неиспользуемые строки, потому что _передача данных в сети_ -- ==дорогостоящая операция==.

Таким образом при обращении к БД можно запросить не всю таблицу целиком, а какой-то ее _фрагмент_ (параметр `query` в `.options()`).
```python
# run.py

from pathlib2 import Path
from pyspark.sql import SparkSession

_HOME = Path().home()
_POSTGRES_JDBC_PATH = "Documents/postgres-jdbc/postgresql-42.7.3.jar"
POSTGRES_JDBC_ABSPATH = str(_HOME.joinpath(_POSTGRES_JDBC_PATH).absolute())
DB_URL = "jdbc:postgresql://localhost:5432/spark_labs"

# Здесь указываем SQL-запрос, чтобы извлечь не всю таблицу, а лишь несколько строк
options = {
	"url": DB_URL,
	"driver": "org.postgresql.Driver",
	"user": "postgres",
	"query": "SELECT * FROM ch02 WHERE lastname ~* '^fin.*$'",  # <= NB
}

# Для того чтобы можно было работать с PostgreSQL, обязательно следует задать `spark.jars`
def main():
	spark = SparkSession \
	    .builder \
	    .appName("Work with PostgreSQL") \
	    .master("local[*]") \
	    .config( 
	        "spark.jars",
			POSTGRES_JDBC_ABSPATH 
	    ) \
	    .getOrCreate()
	
	df = spark.read.format("jdbc").options(**options).load()


if __name__ == "__main__":
    main()
    
```

В том случае, когда приложение запускается как Python-сценарий (то есть не в интерактивной сессии), путь до JAR-файла (JDBC-драйвера PostgreSQL) в `.config("spark.jars", ...)` можно не указывать, но обязательно его нужно указать при запуске с помощь `spark-submit`
```bash
$ export SPARK_CLASSPATH=~/Documents/postgres-jdbc/postgresql-42.7.3.jar
$ spark-submit --driver-class-path ${SPARK_CLASSPATH} run.py
# Или можно использовать флаг --jars
$ spark-submit --jars ${SPARK_CLASSPATH} run.py
```

При работе со Spark в интерактивном режиме (`ipython`), требуется в методе `.config("spark.jars", ...)` передать путь до JDBC-драйвера
```python
...
spark = SparkSession \
	.builder \
	.appName("Work with PostgreSQL") \
	.master("local[*]") \
	.config("spark.jars", POSTGRES_JDBC_ABSPATH) \  
	.getOrCreate()
```

Если же все-таки требуется извлечь все записи таблицы, то в `.options()` необходимо указать ключ `dbtable` и передать ему имя интересующей таблицы.
```python
from pathlib2 import Path
from pyspark.sql import SparkSession

_HOME = Path().home()
_POSTGRES_JDBC_PATH = "Documents/postgres-jdbc/postgresql-42.7.3.jar"
POSTGRES_JDBC_ABSPATH = str(_HOME.joinpath(_POSTGRES_JDBC_PATH).absolute())
DB_URL = "jdbc:postgresql://localhost:5432/spark_labs"

# Здесь указываем SQL-запрос, чтобы извлечь не всю таблицу, а лишь несколько строк
options = {
	"url": DB_URL,
	"driver": "org.postgresql.Driver",
	"user": "postgres",
    "dbtable": "ch02",  # <= NB
}

# Для того чтобы можно было работать с PostgreSQL, обязательно следует задать `spark.jars`
def main():
	spark = SparkSession \
	    .builder \
	    .appName("Work with PostgreSQL") \
	    .master("local[*]") \
	    .config(  # <= NB
	        "spark.jars",
			POSTGRES_JDBC_ABSPATH 
	    ) \
	    .getOrCreate()
	
	df = spark.read.format("jdbc").options(**options).load()


if __name__ == "__main__":
    main()
    
```

### Соединение данных в базе данных

Используя похожую методику, можно выполнять соединение (join) данных в БД _перед началом_ операции потребления в Spark. Spark может соединять данные между кадрами данных, но из соображений улучшения производительности и оптимизации, возможно, потребуется выполнение запроса к БД на выполнение операции соединения.

SQL-запрос отправляется _непосредственно в СУБД_ (например, MySQL, PostgreSQL etc.) и _не интерпретируется в Spark_, следовательно, если вы пишете специализированный код Oracle SQL, то он не будет работать с PostgreSQL. 

### Параллельное чтение из баз данных

Детали можно найти на странице [Read JDBC in Parallel using PySpark] (https://sparkbyexamples.com/pyspark/read-jdbc-in-parallel-using-pyspark/)

Если задать значение параметра `numPartitions` цепочечного метода `.option()`, то _читать_ таблицу из базы данных или _записывать_ таблицу в базу данных можно будет _параллельно_. 

В документации говориться, что `numPartitions` это максимальное количество разделов, которые можно использовать для организации параллельного режима при чтении и записи таблицы. Это также определяет максимальное количество параллельных соединений с JDBC.

Для чтения таблицы из базы данных можно использоваться связку `.format("jdbc").options(...).load()` или `.format("jdbc").option(<key>, <value>)...option(<key>, <value>).load()`
```python
# Read Table in Parallel mode
df = spark.read.format("jdbc") \
    .option("driver", "com.mysql.cj.jdbc.Driver") \
    .option("url", "jdbc:mysql://localhost:3306/emp") \
    .option("dbtable", "employee") \
    .option("numPartitions", 5) \
    .option("user", "...") \
    .option("password", "...") \
    .load()
```

Также можно указать отдельные атрибуты (столбцы) в параметре `query`
```python
df = spark.read.format("jdbc") \
    .option("driver", "com.mysql.cj.jdbc.Driver") \
    .option("url", "jdbc:mysql://localhost:3306/emp") \
    .option("query", "SELECT id, age FROM employee WHERE gender = 'M'") \
    .option("numPartitions", 5) \
    .option("user", "...") \
    .option("password", "...") \
    .load()

print(df.rdd.getNumPartitions())  # 5
```

NB: Если используется параметр `query`, то нельзя одновременно использовать параметр `partitionColumn`.

Кроме того можно задать параметр `partitionColumn` для указания атрибута, по которому будет выполняется партиционирование. Если используется параметр `partitionColumn`, то обязательно требуется определить и параметры `lowerBound`, `upperBound`, а также `numPartitions`.

Следует обратить внимание, что параметры `lowerBound`,  `upperBound` и `numPartitions` определяют диапазон значений атрибута, по которому выполняется партиционирование. Например, если `numPartations = 10`, `lowerBound = 0`, а `upperBound = 1000`, то размер каждой группы записей будет определяться как `(upperBound - lowerBound) / numPartitions`, то есть в данном случае `(1000 - 0) / 10 = 100`
```sql
SELECT * FROM table WHERE partitionColumn BETWEEN 0 AND 100
SELECT * FROM table WHERE partitionColumn BETWEEN 100 AND 200
SELECT * FROM table WHERE partitionColumn BETWEEN 200 AND 300
...
SELECT * FROM table WHERE partitionColumn BETWEEN 900 AND 1000
```



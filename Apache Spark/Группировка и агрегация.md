Полезная ссылка https://www.statology.org/pyspark-groupby-agg-multiple-columns/ 

Если требуется посчитать агрегаты для одного столбца, то можно поступить так
```python
df.groupBy("group").agg({"col1": "mean"})
```

Если же требуется для каждого из нескольких столбцов посчитать разные агрегаты, то можно использовать функции Spark
```python
from pyspark.sql import functions as F

df.groupBy("group").agg(
	F.sum("col1").alias("my_sum"),
	F.mean("col1"),
	F.min("col2").alias("my_min"),
	F.max("col2"),
)
```

Для того чтобы построить сводку с простыми метриками, можно воспользоваться комбинацией `.select()` и `.agg()`
```python
# PySpark
df.select("col1", "col2").agg(
	F.mean("col1"),
	F.std("col1"),
	F.mean("col2"),
	F.min("col2")
).show(vertical=True)
```

Еще можно передать распакованную коллекцию методу `.agg()` 
```python
df.agg(*(
    F.mean("x"),
    F.stddev("x"),
    F.min("y"),
    F.mean("y")
)).show()
```

Или просто передать неупакованный набор агрегатов (без распаковки), так как метод `.agg()` принимает varargs -- то есть произвольное число аргументов
```python
df.agg(
	F.mean("x"),
	F.stddev("x"),
	F.mean("y"),
	F.min("y")
).show()
```

В Pandas эту задачу можно решить так
```python
# Pandas
df[["col1", "col2"]].agg(
	{
        "col1": ["mean", "std"],
        "col2": ["mean", "min"],
	}
).unstack()
```

Полезная ссылка https://www.statology.org/pyspark-groupby-agg-multiple-columns/ 

Если требуется посчитать агрегаты для одного столбца, то можно поступить так
```python
df.groupBy("group").agg({"col1": "mean"})
```

Если же требуется для каждого из нескольких столбцов посчитать разные агрегаты, то можно использовать функции Spark
```python
from pyspark.sql import functions as F

df.groupBy("group").agg(
	F.sum("col1").alias("my_sum"),
	F.mean("col1"),
	F.min("col2").alias("my_min"),
	F.max("col2"),
)
```

Получить распределение значений по категориям можно так
```python
df.groupBy("color").count()
```

Если нужно еще отсортировать записи, то
```python
df.groupBy("color").count() \
    .sort(F.desc("count"))  # или .sort(F.col("count").desc())
```

Дополнительно можно отфильтровать строки гриппировки
```python
df.groupBy("color").count() \
    .sort(F.desc("count")) \
    .filter(F.col("count") >= 3)  # или .filter("`count` >= 3")
```

Для того чтобы построить сводку с простыми метриками, можно воспользоваться комбинацией `.select()` и `.agg()`
```python
# PySpark
df.select("col1", "col2").agg(
	F.mean("col1"),
	F.std("col1"),
	F.mean("col2"),
	F.min("col2")
).show(vertical=True)
```

Или можно просто передать набор агрегатов, так как метод `.agg()` принимает varargs -- то есть произвольное число аргументов
```python
df.agg(
	F.mean("x"),
	F.stddev("x"),
	F.mean("y"),
	F.min("y")
).show()
```

В Pandas эту задачу можно решить так
```python
# Pandas
df[["col1", "col2"]].agg(
	{
        "col1": ["mean", "std"],
        "col2": ["mean", "min"],
	}
).unstack()
```

### Описательные статистики в Spark 

Как известно, медиана в меньшей степени подвержена влиянию выбросов, однако эта метрика очень дорогая с вычислительной точки зрения, так как предполагает сортировку. Spark для ускорения вычисления медианы использует _алгортим Гринвальда-Ханна_ [Greenvald-Khanna quantile estimator](https://aakinshin.net/posts/greenwald-khanna-quantile-estimator/), [Generalizing Greenwald-Khanna Streaming Quantile Summaries for Weighted Inputs](https://arxiv.org/pdf/2303.06288).

Посчитать квантили (например, 1-ый квартиль, медиану и 3-ий квартиль) по алгоритму Гринвальда-Ханна для не пропущенных значений столбца можно так
```python
df.filter(
	~(F.col("x").isNull() | F.isnan(F.col("x")))
).approxQuantile("x", [0.25, 0.5, 0.75], 1e-03)
# [-1.1945987315009814, -0.5523844873047099, 0.48526258805629807]
```

Получить количество уникальных значений атрибута можно так
```python
df.select(F.countDistinct("color")).show()
```

А просто вывести уникальные значения атрибута (включая `NULL`) можно так
```python
df.select("color").distinct().show()
```

Стратифицированное семплирование можно выполнить с помощью метода `.sampleBy()`
```python
df.sampleBy("colors", fractions={1959: 0.2, 1960: 0.4, 1961: 0.4}, seed=42)
```

Найти приближенное число уникальных значений для каждого атрибута кадра данных можно так
```python
df.select([
    F.approx_count_distinct(col_).alias(f"{col_}_n_uniq")
    for col_ in df.columns
]).show()
```

Удалить из кадра данных константные стоблцы (то есть столбцы с одним уникальным значением) можно так
```python
df_without_const_cols = df.drop(
	*[key for key, value in df.select([
		F.approx_count_distinct(col_).alias(col_)
		for col_ in df.columns
	]).first().asDict().items() if value == 1])
```

Код страшноватый, поэтому его можно спрятать в функцию
```python
from pyspark.sql.dataframe import DataFrame as SparkDataFrame

def get_dataframe_without_cols_set_cardinality(
	df: SparkDataFrame,
	cardinality: int = 1,
) -> SparkDataFrame:
    """
	Возвращает кадр данных без столбцов
	с указанным значением кардинальности
    """

	return df.drop(
		*[key for key, value in df.select([
			F.approx_count_distinct(col_).alias(col_)
			for col_ in df.columns
		]).first().asDict().items() if value == cardinality])
    
```

Удалить строки, содержащие `NULL` можно с помощью `.na` (возвращает строки, содержащие `NULL`)
```python
df = df.na.drop()
```

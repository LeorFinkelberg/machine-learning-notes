Обычно потоки доступны в двух формах:
- файлы (файловая потоковая обработка),
- сетевые потоки

При варианте файловой потоковой обработки файлы помещаются в каталог, а Spark берет файлы из этого каталога по мере их поступления. Во втором варианте данные передаются по сети.

Spark выполняет потоковую обработку особым образом: перегруппировка операций в небольшом временном окне. Этот способ называют микропакетированием (microbatching).

### Файловая потоковая обработка

Создание сеанса Spark выполняется одинаково как при использовании потока, так и для пакетной обработки данных. После создания сеанса можно приказать сеансу начать операцию чтения из потока с использованием
```python
spark.readStream.format("text").load("/tmp/")
```
Создается сеанс, затем, чтобы создать кадр данных, начинается чтение данных из потока. Но в потоке может и не оказаться данных, они пока еще не были переданы в поток. Следовательно, приложение должно ждать, когда данные появятся в потоке.

Простой пример
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructTrype, StructField, StringType, IntegerType
import logging

logging.debug("-> start")

# Creates a session on a local master
spark = SparkSession.builder.appName("Read lines from a file stream") \
    .master("local[*]").getOrCreate()

schema = StructType([
	StructField("fname", StringType(), True),
	StructField("mname", StringType(), True),
	StructField("lname", StringType(), True),
	StructField("age", IntegerType(), True),
	StructField("ssn", StringType(), True),
])

# Создаем СТРУКТУРИРОВАННЫЙ поток
df = spark.readStream.format("csv") \
    .schema(schema) \  # распарсит строку в соответствие со схемой
    .load("/tmp/")  # потребляет файловые данные из директории `/tmp/`
# Use below for Windows
# .load("C:/tmp/")

df = df.withColumn(
	"result", 
    F.concat(F.col("fname"), F.lit("-"), F.col("age"))
)

query = df.writeStream.outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .option("numRows", 3) \
    .start()

if not query.awaitTermination(timeout=60):  # ждет 60 секунд
    query.stop()

logging.debug("<- end")
```

Здесь данные выводятся в консоли (`.format("console")`). При выводе в консоль установка значения `False` для параметра `.option("truncate", False)` означает, что записи не будут усекаться до определенной длины, а параметр `numRows` определяет вывод не более трех записей. Это равнозначно вызову метода `.show(3, False)` в кадре данных.

Разумеется, прямо сейчас приложение должно ждать поступления данных. Это делается с помощью `.awaitTermination(timeout=60)` в самом запросе. Метод `.awaitTermination()` -- это блокирующий метод, который вызывается как с параметрами, так и без параметров. Без параметров этот метод будет ждать вечно. С помощью параметров можно определить продолжительность интервала ожидания.

Запускаем приложение
```bash
$ spark-submit path-to-stream-file.py
```

После запуска приложения Spark начинает следить за содержимым директории `/tmp/`. Здесь предполагается, что в фоне работает какой-то генератор данных, который дописывается строки в какой-то файл, расположенный в этой директории `/tmp/`. Если в директорию положить один файл с несколькими строками, то Spark потребит этот файл первым пакетом (`Batch: 0`) с парсингом строки в соответствие с указанной схемой и станет дожидаться, когда закончится таймаут.

Еще один пример файловой потоковой обработки [[Литература#^65f453]]<p. 281>. Создаем директорию, в которую будут складываться CSV-файлы (например, `source-data`) . Пока эта директория пустая. Создаем несколько файлов вида 
```bash
# data_temp1.csv
day,temp_in_celsius
day1,12.2
day2,13.1
...
```
Предполагается, что эти файлы приходят из какого-то сервиса (пока директория `source-data` пустая).

Пишем Spark-приложение
```python
from pyspark.sql.types import *

schema = StructType([
    StructField("day", StringType(), True),
    StructField("temp_in_celsius", StringType(), True),
])

df = spark.readStream \
    .option("sep", ",") \
    .schema(schema) \
    .csv("./source-data")

query = df \
    .writeStream \
    .format("console") \
    .outputMode("append") \
    .start()  # обязательно нужно запустить

# ждем 60 секунд, а потом закрываем поток
if not query.awaitTermination(60):  
    query.stop()
```

Теперь если положить файл `temp_data1.csv` в директорию `source-data`, то в консоли появится вывод
```bash
-----------------------------
Batch: 0
-----------------------------
+---+---------------+
|day|temp_in_celsius|
+---+---------------+
|day1|          12.2|
|day2|          13.1|
...
```

Если в директорию `source-data` положить копию файла `temp_data1.csv`, то ничего не произойдет, так как Spark _гарантирует однократную доставку_ (exactly-once). То есть никакие данные не будут утеряны и никакие данные не будут доставлены более одного раза (каждая запись обрабатывается _строго один раз_).

### Запись обработанных потоковых данных в директорию

Пример записи потокового кадра данных в директорию 
```python
# file-stream.py

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import *

def main():
    schema = StructType([
        StructField("day", StringType(), True),
        StructField("temp_in_celsius", StringType(), True),
    ])

    spark = SparkSession.builder \
        .appName("file-stream") \
        .master("local[*]") \
        .getOrCreate()

    df = spark.readStream \
        .format("csv") \
        .option("sep", ",") \
        .schema(schema) \
        .load("./source-data")

    # Простая фильтрация
    df = df.filter(F.col("temp_in_celsius").astype("float") > 13)

    query = df.writeStream \
        .format("csv") \
        .option("path", "./output-from-spark") \ # Результаты фильтрации
        .option("checkpointLocation", "./check-points") \ # Обязательно!!!
        .outputMode("append") \
        .start()

    if not query.awaitTermination(30):
        query.stop()
        spark.stop()


if __name__ == "__main__":
    main()
```

По истичении 30 секунд приложение остановится. В директории запуска будут созданы поддиректории `check-points` и `output-from-spark`. 

Директория `output-from-spark` имеет следующий вид
```bash
├── [ 256]  _spark_metadata
│   ├── [ 241]  0
│   ├── [ 241]  1
│   └── [ 241]  2
├── [  28]  part-00000-9b5d07b4-a54b-4e53-a3aa-57684a101478-c000.csv
├── [  30]  part-00000-a6f2538f-5fae-4ab6-a756-eb73afe319ed-c000.csv
└── [  40]  part-00000-f07c3bd6-90ad-4e5f-8c2c-28473088751d-c000.csv
```
содержит результаты фильтрации по атрибуту `temp_in_celsius`.

А директория `check-points` содержит информацию по 
```bash
├── [ 256]  commits
│   ├── [  29]  0
│   ├── [  29]  1
│   └── [  29]  2
├── [  45]  metadata
├── [ 256]  offsets
│   ├── [ 656]  0
│   ├── [ 656]  1
│   └── [ 656]  2
└── [  96]  sources
    └── [ 256]  0
        ├── [ 117]  0
        ├── [ 117]  1
        └── [ 117]  2
```

Каждый файл `./check-points/sources/0/*` содержит информацию о пути исходного файла, временной метке и номере батча. Например для 2-ого батча
```bash
# ./check-points/sources/0/2 
v1
{"path":"file:///Users/leor.finkelberg/GARBADGE/source-data/temp_data3.csv","timestamp":1719018112395,"batchId":2}
```

Файлы из поддиректории `./check-points/offsets/0/` имеют вид
```bash
# ./check-points/offsets/1
v1
{
  "batchWatermarkMs":0,
  "batchTimestampMs":1719023340815,
  "conf": {
"spark.sql.streaming.stateStore.providerClass":
"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider",
"spark.sql.streaming.join.stateFormatVersion":"2",
"spark.sql.streaming.stateStore.compression.codec":"lz4",
"spark.sql.streaming.stateStore.rocksdb.formatVersion":"5",
"spark.sql.streaming.statefulOperator.useStrictDistribution":"true",
"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion":"2",
"spark.sql.streaming.multipleWatermarkPolicy":"min",
"spark.sql.streaming.aggregation.stateFormatVersion":"2",
"spark.sql.shuffle.partitions":"200"}
}
{"logOffset":1}
```

### Запись обработанных потоковых данных из Kafka на файловую систему

Кое-какие детали по поводу `maxRecordsPerFile` можно найти здесь  https://stackoverflow.com/questions/60654782/pyspark-writestream-each-data-frame-row-in-a-separate-json-file

```python
spark = SparkSession.builder.appName("...").getOrCreate()

sdf = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "test") \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load() \
    .select(
        from_json(
            col("value").cast("string"),
            json_schema
        ).alias("parsed_value")
    )

_sdf = sdf.select("parsed_value.*")
```

Каждый `_sdf` будет записываться в отдельный JSON-файл
```python
_sdf.writeStream \
    .format("json") \
    .option("path", "./output-from-spark") \
    .option("checkpointLocation", "./check-points") \
    .option("maxRecordsPerFile", 1) \
    .start()
```

Для Spark потребуется каталог контрольных точек, чтобы сохранять промежуточные состояния и контрольные точки. Можно задать этого каталог для каждого конкретного потока вывода или глобально на уровне сеанса `SparkSession`, используя для этого [[Литература#^61ef72]]<c. 601>
```python
SparkSession.conf.set("spark.sql.streaming.checkpointLocation", ...)
```

Между различными выполнения этого приложения необходимо очистить каталог контрольных точек!

Примеры использования различных получателей (вывод в файл, вывод в Kafak etc.) можно найти в [[Литература#^61ef72]]<c. 599>.

Вывод в топик Kafka выглядит так
```python
query = df.writeStream \
    .outputMode("update") \
    .format("kafka") \
    .option("kafka.bootrstap.servers", "host1:port1,host2:port2") \
    .option("topic", "updates") \
    .start()
```
### Потребление данных из сетевых потоков

Данные могут также поступать из сетевого потока. Механизм обработки _структурированных потоков_ Spark может обрабатывать _сетевой поток_ так же просто, как и _файловый поток_. 

Для создания сетевого потока потребуется небольшое инструментальное средство NetCat (`nc`). Для ОС Linux Manjaro придется установить `gnu-netcat`
```bash
$ pacman -S gnu-netcat
```

Программа `nc` будет работать как сервер, используя порт `9999`. Запускать утилиту `nc` необходимо _до запуска_ всех прочих программ.
```bash
$ nc -lk 9999
```

Ключ `-l` определяет, что утилита `nc` должна работать в _режиме прослушивания_, а ключ `-k` позволяет устанавливать несколько соединений. После инициализации `nc` можно запускать приложение Spark.

NB: на ОС Manjaro утилита `nc` не поддерживает ключ `-k`. Запускать NetCat в режиме прослушивания на локальном хосте с портом `9999` следует так
```bash
$ nc -l -p 9999
```

Сначала нужно запустить утилиту `nc`, а затем приложение Spark. После запуска приложения Spark можно вернуться в терминал, где работает `nc`, и начать ввод в окне этого терминала. Например, можно ввести строку `Jean-George Perrin` в терминале `nc` и ее отобразит Spark.
```python
from pyspark.sql import SparkSession, function as F

spark = SparkSession.builder \
    .appName("Read lines over a network stream") \
    .master("local[*]") \
    .getOrCreate()

df = spark.readStream.format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# `value` это имя столбца в кадре данных 
df = df.writeColumn(
	"value",
	F.split(F.col("value"), ",").getItem(1)
)

query = df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```

Если в терминале `nc` написать и отправить `Leor Finkelberg`, то Spark (если логика работы приложения не предполагает каких-либо изменений) выведет
```bash
...
+---------------+
|          value|
+---------------+
|Leor Finkelberg|
+---------------+
```

Поэтому к столбцу с именем `value` можно обращаться внутри приложения для выполнения различных преобразований.

### Работа с несколькими потоками

Spark может потреблять данные из нескольких потоков одновременно и совместно использовать один и тот же процесс обработки данных. 

Для обеспечения работы приложения необходимо одновременно запустить два генератора записей. Каждый генератор запускается в отдельном терминале.

Здесь уже не используется метод `.awaitTermination()` , так как это блокирующая операция. Используется цикл для проверки активности потока данных с помощью метода `.isActive()`.

Пример
```python
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import *

TIMEOUT = 10  # seconds
SCHEMA = StructType([
	StructField("fname", StringType(), True),
	StructField("mname", StringType(), True),
	StructField("lname", StringType(), True),
	StructField("age", IntegerType(), True),
	StructField("ssn", StringType(), True),
])

spark = SparkSession.builder \
    .appName("Two Streams") \
    .master("local[*]") \
    .getOrCreate()

# --- FileStream-1 ---
df1 = spark.readStream.format("csv") \
    .schema(SCHEMA) \
    .load("/tmp/data/subdir1/")
df1 = df1.withColumn("fname", F.upper(F.col("fname")))

query1 = df1.writeStream.outputMode("append") \
    .format("console") \
    .option("numRows", 3) \
    .start()
# --- FileStream-1 ---

# --- FileStream-2 ---
df2 = spark.readStream.format("csv") \
    .schema(SCHEMA) \
    .load("/tmp/data/subdir2/")
df2 = df2.withColumn("fname", F.lower(F.col("fname")))

query2 = df2.writeStream.outputMode("append") \
    .format("console") \
    .option("numRows", 3) \
    .start()
# --- FileStream-2 ---

start_plus_timeout = datetime.now() + timedelta(TIMEOUT)
while query1.isActive and query2.isActive:
    if start_plus_timeout < datetime.now():
        query1.stop()
        query2.stop()
```

### Различия между дискретизированными и структурированными потоками

Spark предоставляет два способа организации потоков. До настоящего момента рассматривались только _структурированные потоки_. Но в Spark изначально существовал другой тип потоков -- _дискретизированные потоки_ (discretized streaming, DStream).

Spark v1.X использовал _дискретизированные потоки_, основанные на _RDD_. Но сейчас в Spark v2.X основой является _кадр данных_. Такой переход существенно улучшил производительность.

Предполагается, что DStream API в какой-то момент будет оставаться постоянно стабильным или будет объявлен устаревшим.
Обычно потоки доступны в двух формах:
- файлы (файловая потоковая обработка),
- сетевые потоки

При варианте файловой потоковой обработки файлы помещаются в каталог, а Spark берет файлы из этого каталога по мере их поступления. Во втором варианте данные передаются по сети.

Spark выполняет потоковую обработку особым образом: перегруппировка операций в небольшом временном окне. Этот способ называют микропакетированием (microbatching).

### Файловая потоковая обработка

Создание сеанса Spark выполняется одинаково как при использовании потока, так и для пакетной обработки данных. После создания сеанса можно приказать сеансу начать операцию чтения из потока с использованием
```python
spark.readStream.format("text").load("/tmp/")
```
Создается сеанс, затем, чтобы создать кадр данных, начинается чтение данных из потока. Но в потоке может и не оказаться данных, они пока еще не были переданы в поток. Следовательно, приложение должно ждать, когда данные появятся в потоке.

Простой пример
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructTrype, StructField, StringType, IntegerType
import logging

logging.debug("-> start")

# Creates a session on a local master
spark = SparkSession.builder.appName("Read lines from a file stream") \
    .master("local[*]").getOrCreate()

schema = StructType([
	StructField("fname", StringType(), True),
	StructField("mname", StringType(), True),
	StructField("lname", StringType(), True),
	StructField("age", IntegerType(), True),
	StructField("ssn", StringType(), True),
])

# Создаем СТРУКТУРИРОВАННЫЙ поток
df = spark.readStream.format("csv") \
    .schema(schema) \  # распарсит строку в соответствие со схемой
    .load("/tmp/")  # потребляет файловые данные из директории `/tmp/`
# Use below for Windows
# .load("C:/tmp/")

df = df.withColumn(
	"result", 
    F.concat(F.col("fname"), F.lit("-"), F.col("age"))
)

query = df.writeStream.outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .option("numRows", 3) \
    .start()

query.awaitTermination(timeout=60)  # ждет 60 секунд

logging.debug("<- end")
```

Здесь данные выводятся в консоли (`.format("console")`). При выводе в консоль установка значения `False` для параметра `.option("truncate", False)` означает, что записи не будут усекаться до определенной длины, а параметр `numRows` определяет вывод не более трех записей. Это равнозначно вызову метода `.show(3, False)` в кадре данных.

Разумеется, прямо сейчас приложение должно ждать поступления данных. Это делается с помощью `.awaitTermination(timeout=60)` в самом запросе. Метод `.awaitTermination()` -- это блокирующий метод, который вызывается как с параметрами, так и без параметров. Без параметров этот метод будет ждать вечно. С помощью параметров можно определить продолжительность интервала ожидания.

Запускаем приложение
```bash
$ spark-submit path-to-stream-file.py
```

После запуска приложения Spark начинает следить за содержимым директории `/tmp/`. Здесь предполагается, что в фоне работает какой-то генератор данных, который дописывается строки в какой-то файл, расположенный в этой директории `/tmp/`. Если в директорию положить один файл с несколькими строками, то Spark потребит этот файл первым пакетом (`Batch: 0`) с парсингом строки в соответствие с указанной схемой и станет дожидаться, когда закончится таймаут.

### Потребление данных из сетевых потоков

Данные могут также поступать из сетевого потока. Механизм обработки _структурированных потоков_ Spark может обрабатывать _сетевой поток_ так же просто, как и _файловый поток_. 

Для создания сетевого потока потребуется небольшое инструментальное средство NetCat (`nc`). Для ОС Linux Manjaro придется установить `gnu-netcat`
```bash
$ pacman -S gnu-netcat
```

Программа `nc` будет работать как сервер, используя порт `9999`. Запускать утилиту `nc` необходимо _до запуска_ всех прочих программ.
```bash
$ nc -lk 9999
```

Ключ `-l` определяет, что утилита `nc` должна работать в _режиме прослушивания_, а ключ `-k` позволяет устанавливать несколько соединений. После инициализации `nc` можно запускать приложение Spark.

NB: на ОС Manjaro утилита `nc` не поддерживает ключ `-k`. Запускать NetCat в режиме прослушивания на локальном хосте с портом `9999` следует так
```bash
$ nc -l -p 9999
```

Сначала нужно запустить утилиту `nc`, а затем приложение Spark. После запуска приложения Spark можно вернуться в терминал, где работает `nc`, и начать ввод в окне этого терминала. Например, можно ввести строку `Jean-George Perrin` в терминале `nc` и ее отобразит Spark.
```python
from pyspark.sql import SparkSession, function as F

spark = SparkSession.builder \
    .appName("Read lines over a network stream") \
    .master("local[*]") \
    .getOrCreate()

df = spark.readStream.format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# `value` это имя столбца в кадре данных 
df = df.writeColumn(
	"value",
	F.split(F.col("value"), ",").getItem(1)
)

query = df.writeStream.outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```

Если в терминале `nc` написать и отправить `Leor Finkelberg`, то Spark (если логика работы приложения не предполагает каких-либо изменений) выведет
```bash
...
+---------------+
|          value|
+---------------+
|Leor Finkelberg|
+---------------+
```

Поэтому к столбцу с именем `value` можно обращаться внутри приложения для выполнения различных преобразований.

### Работа с несколькими потоками

Spark может потреблять данные из нескольких потоков одновременно и совместно использовать один и тот же процесс обработки данных. 

Для обеспечения работы приложения необходимо одновременно запустить два генератора записей. Каждый генератор запускается в отдельном терминале.

Здесь уже не используется метод `.awaitTermination()` , так как это блокирующая операция. Используется цикл для проверки активности потока данных с помощью метода `.isActive()`.

Пример
```python
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.types import *

TIMEOUT = 10  # seconds
SCHEMA = StructType([
	StructField("fname", StringType(), True),
	StructField("mname", StringType(), True),
	StructField("lname", StringType(), True),
	StructField("age", IntegerType(), True),
	StructField("ssn", StringType(), True),
])

spark = SparkSession.builder \
    .appName("Two Streams") \
    .master("local[*]") \
    .getOrCreate()

# --- FileStream-1 ---
df1 = spark.readStream.format("csv") \
    .schema(SCHEMA) \
    .load("/tmp/data/subdir1/")
df1 = df1.withColumn("fname", F.upper(F.col("fname")))

query1 = df1.writeStream.outputMode("append") \
    .format("console") \
    .option("numRows", 3) \
    .start()
# --- FileStream-1 ---

# --- FileStream-2 ---
df2 = spark.readStream.format("csv") \
    .schema(SCHEMA) \
    .load("/tmp/data/subdir2/")
df2 = df2.withColumn("fname", F.lower(F.col("fname")))

query2 = df2.writeStream.outputMode("append") \
    .format("console") \
    .option("numRows", 3) \
    .start()
# --- FileStream-2 ---

start_plus_timeout = datetime.now() + timedelta(TIMEOUT)
while query1.isActive and query2.isActive:
    if start_plus_timeout < datetime.now():
        query1.stop()
        query2.stop()
```

### Различия между дискретизированными и структурированными потоками

Spark предоставляет два способа организации потоков. До настоящего момента рассматривались только _структурированные потоки_. Но в Spark изначально существовал другой тип потоков -- _дискретизированные потоки_ (discretized streaming, DStream).

Spark v1.X использовал _дискретизированные потоки_, основанные на _RDD_. Но сейчас в Spark v2.X основой является _кадр данных_. Такой переход существенно улучшил производительность.

Предполагается, что DStream API в какой-то момент будет оставаться постоянно стабильным или будет объявлен устаревшим.
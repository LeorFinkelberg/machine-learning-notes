_Хранилище данных_ может быть размещено _в памяти_ или _на диске_. Это зависит от стратегии Spark на текущий момент времени, но во всех случаях, когда это возможно, используется _память_ [[Литература#^61ef72]]<c. 75>.

Фреймы данных, наборы данных и устойчивые распределенные наборы данных (RDD) считаются неизменяемым хранилищем данных. Неизменяемость (immutability) определяется как невозможность внесения изменений (unchangeable). В применении к любому объекту это означает, что его состояние не может быть изменено после того, как объект создан.

Spark сохраняет только этапы преобразований, а не данные, преобразованные на каждом шаге. Spark сохраняет _начальное состояние данных_  в ==неизменяемой== форме, а затем сохраняет "предписание" (recipe) (_список выполняемых преобразований_). Промежуточные (преобразованные) данные не сохраняются [[Литература#^61ef72]]<c.76>.

Другими словами, на начальном этапе данные сохраняются в неизменяемой форме. Далее сохраняются _только указания по преобразованиям_, но не данные, получаемые в результате отдельных этапов преобразований.

Список некоторых методов и функций для работы кадрами данных:
- `.withColumn()`:  создает новый столбец из заданного выражения или столбца
```python
df = df.withColumn(
	"full_name",
	F.concat(F.col("name"), F.lit("_"), F.col("lastname"))
)
```
- `.withColumnRenamed()`: выполняет переименование столбца
```python
df = df.withColumnRenamed("full_name", "fname")
```
- `.withColumnsRenamed()`: выполняет переименование столбцов
```python
df = df.withColumnsRenamed(
	{
        "col1": "col_1",
        "col2": "col_2",
	}
)
```
- `F.col("col_name")`: возвращает объект столбца `pyspark.sql.column.Column`
- `.drop("col_name")`: возвращает кадр данных без указанного столбца
- `F.lit()`: создает столбец с заданным значением
```python
df = df.withColumn("value", F.lit(10))
```
- `F.concat()`: объединяет значения в набор столбцов

Методы можно вызывать цепочкой. Например так
```python
df = df \
    .withColumn("value", F.lit("foo")) \
    .withColumnRenamed("lname", "last_name") \
    .withColumnsRenamed({
		 "full_name": "fname",
		 "last_name": "lname",
    }) \
    .drop("value")
```

Преобразовать Spark кадр данных в кадр данных Pandas можно так
```python
# NB! .to_pandas() загружает все данные в память драйвера! Поэтому пользоваться нужно осторожно, в предположении, что итоговый кадр данных небольшой и может поместиться в памяти драйвера
df_pd: pd.DataFrame = df.pandas_api().to_pandas()
```

Метод `DataFrame.to_pandas_on_spark()` считается устаревшим! Вместо него следует использовать `DataFrame.pandas_api()`.

На чистом Pandas эту задачу можно было бы решить так
```python
df["value"] = "foo"
df = df \
    .rename(columns={"lname": "last_name"}) \
    .rename(columns={"full_name": "fname", "last_name": "lname"}) \
    .drop(["value"], axis=1)
```

После загрузки данных можно посмотреть, где они хранятся. Это позволит узнать, как организовано внутреннее хранение данных в Spark. Физически данные хранятся НЕ в кадре данных, а в разделах (partitions).

К разделам нельзя получить доступ непосредственно из кадра данных. Разделы необходимо рассматривать "через призму" устойчивых распределенных наборов данных (RDD). Данные хранятся в _разделах_.

Количество разделов можно получить так
```python
df.rdd.getNumPartitions()  # 1
```

Изменить количество разделов можно с помощью метода `.repartition()`. Изменение организации разделов может увеличить производительность
```python
df = df.repartition(4)
df.rdd.getNumPartitions()  # 4
```

Можно посмотреть схему кадра данных
```python
df.schema  # StructType([StructField('name', StringType(), True), ...])
schema_ = df.schema
schema_.jsonValue()
```

Прочитать JSON-файл можно аналогичным способом
```python
df = spark.read.format("json").load("./Restaurants_in_Durham_Country_NC.json")
```

Для доступа к полям структуры можно использовать символ точки (`.`) в пути. Для доступа к элементу в массиве используется метод `.getItem()`.
```python
# F.col("path.to.attr")
df = df \
    .withColumn("datasetId", F.col("fields.id")) \
    .withColumn("type", F.split(F.col("fields.type_description"), "-").getItem(1)) \
    .withColumn("dateStart", F.col("fields.opeing_date")) \
    .withColumn("geoX", F.col("fields.geolocation").getItem(0))
```

Посмотрим значения всех записей кадра данных по пути `fields.type_description`
```python
data.select(F.col("fields.type_description")).show()
```

В Pandas атрибуты `type`, `geoX`, `geoY` можно было бы получить так
```python
import typting as t

data = pd.read_csv("./Restaurants...json")

# Метод .apply применяет указанную функцию к каждому элементу столбца (Series) `fields`
fields_type = t.Union[str, int, t.List[float]]
get_type_description: t.Callable[[t.Dict[str, fields_type]], str] = \
    lambda record: record.get("type_description").split("-")[1]
data["type"] = data["fields"].apply(get_type_description)

data["geoX"] = data["fields"] \
    .apply(lambda record: record.get("geolocation")[0])

data["geoY"] = data["fields"] \
    .apply(lambda record: record.get("geolocation")[1])
```

Следует отметить, что при _удалении родительского столбца_ также удаляются _все вложенные в него столбцы_. Столбцы, вложенные в поля `fields` и `geometry`, удаляются, потому что были удалены родительские столбцы. Поэтому при удалении столбца `fields` все его подстолбцы, такие как `risk`, `seats`, `sewage` и т.д., одновременно удаляются.
```python
df = df.withColumn("country", F.lit("Durham")) \
    .withColumn("datasetId", F.col("fields.id")) \
    ...
    .drop("fields") \
    .drop("geometry")
    ...
```

При загрузке _небольшого_ (обычно не более 128 Мб) набора данных в кадр данных Spark создает _только один раздел_ . Но в рассматриваемом случае Spark создает раздел для набора данных на основе формата CSV и другой раздел для набора данных на основе формата JSON. 
В итоге для двух наборов данных в различных кадрах данных создается не менее двух разделов.

Склеить два кадра данных Spark по оси строк (в Pandas эта задача решается с помощью `pd.concat([df1, df2])` можно следующим образом. Но важно, чтобы схемы кадров данных совпадали
```python
df1.unionByName(df2)
```

Вычислить число $\pi$ методом Монте-Карло можно так
```python
from time import perf_counter
from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder.appName("Compute Pi").master("local[*]").getOrCreate()

def comupte_pi(
	N: int = 1_000_000,
	random_seed_x: int = 42,
	random_seed_y: int = 54,
    explain: bool = False,
) -> float:
    """Calculates Pi using Monte-Carlo method"""
    
    start = perf_counter()

	df = spark.range(N) \
	    .withColumn("x", F.rand(random_seed_x)) \
	    .withColumn("y", F.rand(random_seed_y)) \
	    .withColumn("dist", F.sqrt(F.col("x") ** 2 + F.col("y") ** 2)) \
	    .withColumn("mask", (F.col("dist") < 1).astype("int"))

    pi = 4 * df.agg({"mask": "sum"}).collect()[0].asDict()["sum(mask)"] / df.count()

    print(f"Total time: {perf_counter() - start:.3f} sec.")

    if explain:
        df.explain()

    return pi
```

Посмотрим план выполнения (читается снизу вверх)
```python
>>> compute(explain = True)

Total time: 0.097 sec.
== Phisical Plan ==
*(1) Project [id#1886L, x#1888, y#1891, dist#1895, cast((dist(#1895 < 1.0) as int) AS mask#1900)]
+- *(1) Project [id#1886L, x#1888, y#1891, SQRT((POWER(x#1888, 2.0) + POWER(y#1891, 2.0))) AS dist#1895]
    +- *(1) Project [id#1886L, x#1888, rand(54) AS y#1891]
        +- *(1) Project [id#1886L, rand(42) AS x#1888]
		    +- *(1) Range (0, 100000, step=1, splits=8)
```

В Spark встроен механизм оптимизации Catalyst. Перед выполнением действия Catalyst просматривает ориентированный ациклический граф (DAG) и улучшает его структуру [[Литература#^61ef72]]<c. 127>. Работа Catalyst похожа на работу оптимизатора запросов с созданием плана запроса в среде реляционной базы данных. Для доступа к плану можно воспользоваться методом кадра данных `.explain()`. Операции располагаются в обратном порядке.

Список _действий_ Spark можно найти на странице документации [здесь](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions), а список _преобразований_ [здесь](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations).

Spark создает список преобразований в форме ориентированного ациклического графа (DAG), который оптимизируется с использованием встроенного в Spark средства оптимизации Catalyst.

Spark работает на уровне столбцов, поэтому нет необходимости в организации итеративного прохода по данным.
_Хранилище данных_ может быть размещено _в памяти_ или _на диске_. Это зависит от стратегии Spark на текущий момент времени, но во всех случаях, когда это возможно, используется _память_ [[Литература#^61ef72]]<c. 75>.

Фреймы данных, наборы данных и устойчивые распределенные наборы данных (RDD) считаются неизменяемым хранилищем данных. Неизменяемость (immutability) определяется как невозможность внесения изменений (unchangeable). В применении к любому объекту это означает, что его состояние не может быть изменено после того, как объект создан.

Spark сохраняет только этапы преобразований, а не данные, преобразованные на каждом шаге. Spark сохраняет _начальное состояние данных_  в ==неизменяемой== форме, а затем сохраняет "предписание" (recipe) (_список выполняемых преобразований_). Промежуточные (преобразованные) данные не сохраняются [[Литература#^61ef72]]<c.76>.

Другими словами, на начальном этапе данные сохраняются в неизменяемой форме. Далее сохраняются _только указания по преобразованиям_, но не данные, получаемые в результате отдельных этапов преобразований.

Список некоторых методов и функций для работы кадрами данных:
- `.withColumn()`:  создает новый столбец из заданного выражения или столбца
```python
df = df.withColumn(
	"full_name",
	F.concat(F.col("name"), F.lit("_"), F.col("lastname"))
)
```
- `.withColumnRenamed()`: выполняет переименование столбца
```python
df = df.withColumnRenamed("full_name", "fname")
```
- `.withColumnsRenamed()`: выполняет переименование столбцов
```python
df = df.withColumnsRenamed(
	{
        "col1": "col_1",
        "col2": "col_2",
	}
)
```
- `F.col("col_name")`: возвращает объект столбца `pyspark.sql.column.Column`
- `.drop("col_name")`: возвращает кадр данных без указанного столбца
- `F.lit()`: создает столбец с заданным значением
```python
df = df.withColumn("value", F.lit(10))
```
- `F.concat()`: объединяет значения в набор столбцов

Методы можно вызывать цепочкой. Например так
```python
df = df \
    .withColumn("value", F.lit("foo")) \
    .withColumnRenamed("lname", "last_name") \
    .withColumnsRenamed({
		 "full_name": "fname",
		 "last_name": "lname",
    }) \
    .drop("value")
```

Преобразовать Spark кадр данных в кадр данных Pandas можно так
```python
# NB! .to_pandas() загружает все данные в память драйвера! Поэтому пользоваться нужно осторожно, в предположении, что итоговый кадр данных небольшой и может поместиться в памяти драйвера
df_pd: pd.DataFrame = df.pandas_api().to_pandas()
```

Метод `DataFrame.to_pandas_on_spark()` считается устаревшим! Вместо него следует использовать `DataFrame.pandas_api()`.

На чистом Pandas эту задачу можно было бы решить так
```python
df["value"] = "foo"
df = df \
    .rename(columns={"lname": "last_name"}) \
    .rename(columns={"full_name": "fname", "last_name": "lname"}) \
    .drop(["value"], axis=1)
```

После загрузки данных можно посмотреть, где они хранятся. Это позволит узнать, как организовано внутреннее хранение данных в Spark. Физически данные хранятся НЕ в кадре данных, а в разделах (partitions).

К разделам нельзя получить доступ непосредственно из кадра данных. Разделы необходимо рассматривать "через призму" устойчивых распределенных наборов данных (RDD). Данные хранятся в _разделах_.

Количество разделов можно получить так
```python
df.rdd.getNumPartitions()  # 1
```

Изменить количество разделов можно с помощью метода `.repartition()`. Изменение организации разделов может увеличить производительность
```python
df = df.repartition(4)
df.rdd.getNumPartitions()  # 4
```

Можно посмотреть схему кадра данных
```python
df.schema  # StructType([StructField('name', StringType(), True), ...])
schema_ = df.schema
schema_.jsonValue()
```

Прочитать JSON-файл можно аналогичным способом
```python
df = spark.read.format("json").load("./Restaurants_in_Durham_Country_NC.json")
```

Для доступа к полям структуры можно использовать символ точки (`.`) в пути. Для доступа к элементу в массиве используется метод `.getItem()`.
```python
# F.col("path.to.attr")
df = df \
    .withColumn("datasetId", F.col("fields.id")) \
    .withColumn("type", F.split(F.col("fields.type_description"), "-").getItem(1)) \
    .withColumn("dateStart", F.col("fields.opeing_date")) \
    .withColumn("geoX", F.col("fields.geolocation").getItem(0))
```

Посмотрим значения всех записей кадра данных по пути `fields.type_description`
```python
data.select(F.col("fields.type_description")).show()
```

В Pandas атрибуты `type`, `geoX`, `geoY` можно было бы получить так
```python
import typting as t

data = pd.read_csv("./Restaurants...json")

# Метод .apply применяет указанную функцию к каждому элементу столбца (Series) `fields`
fields_type = t.Union[str, int, t.List[float]]
get_type_description: t.Callable[[t.Dict[str, fields_type]], str] = \
    lambda record: record.get("type_description").split("-")[1]
data["type"] = data["fields"].apply(get_type_description)

data["geoX"] = data["fields"] \
    .apply(lambda record: record.get("geolocation")[0])

data["geoY"] = data["fields"] \
    .apply(lambda record: record.get("geolocation")[1])
```

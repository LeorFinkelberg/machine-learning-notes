Для больших данных широко используются новые форматы файлов (Avro, ORC, Parquet etc.), так как CSV или JSON уже не оправдывают ожиданий.

Валидные значения для чтения, к примеру, CSV-файла (`sep`, `dateFormat`, `inferSchema` etc.) или для работы с БД (`url`, `numPartitions`, `columnName` etc.) для метода `spark.read.format("...").option()` можно найти на странице [DataFrameReader](https://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.sql.DataFrameReader@csv(paths:String*):org.apache.spark.sql.DataFrame).

### Чтение CSV-файлов

Для того чтобы в pandas прочитать CSV-файл, в котором в качестве кавычек используются символ `*` , в качестве разделителя -- символ `;`, а в столбце времени `releaseDate` (предполагается формат `m/d/Y`) есть пропуски, можно воспользоваться следующей конструкцией
```python
from datetime import datetime

# errors = "coerce" требуется для обработки случаев, когда pandas не может преобразовать значение во временную метку
date_parser = lambda date: pd.to_datetime(date, format="%m/%d/%Y", errors="coerce") 

df = pd.read_csv(
	"./books.csv",
	sep=";",
	quotechar="*",
	parse_dates=["releaseDate"],
	date_parser=date_parser,
	keep_default_na=False,  # чтобы не реагировал на пропуски (NaN) при парсинге даты
    infer_datetime_format=True,
)
```

Следует иметь в виду, что функция `pd.to_datetime()` для "старых" дат (скажем 1960 год) возвращает `NaT`. Например
```python
pd.to_datetime("05/09/1960", format="%m/%d/%Y", errors="coerce")  # Timestamp('1960-05-09 00:00:00')
# Но для 1660 года
pd.to_datetime("05/09/1660", format="%m/%d/%Y", errors="coerce")  # NaT 
```

К слову Polars такие временные метки обрабатывает корректно
```python
import polars as pl

df = pl.read_csv(
	"./books.csv",
	separator=";",
	quote_char="*",
	try_parse_dates=True,
).with_columns(
	pl.col("releaseDate").str.strptime(pl.Date, "%m/%d/%Y")  # -> date
)
```

Функция `datetime.strptime(x, <datetime_template>)` используется для парсинга временной метки, представленной в виде шаблона `<datetime_template>`. Для форматирования результата можно использовать метод `.strftime(<another_datetime_template>)`
```python
datetime.strptime("2024/05/09", "%Y/%m/%d").strftime("%d-%B-%Y")  # 09-May-2024 
```

В документации по параметру `keep_default_na` функции `pd.read_csv()` говорится, что если параметр `keep_default_na=False` и при этом параметр `na_values` не задан, то пропуски не будут рассматриваться парсером.

Если параметр `infer_datetime_format` выставлен в `True` и задан параметр `parse_dates`, то pandas будет пытаться вывести формат на основе данных столбца временных меток и выбрать более быстрый алгоритм парсинга. В некоторых случаях это позволяет увеличить скорость парсинга в 5-10 раз.

В Spark эту задачу можно решить так
```python
df = spark.read.format("csv") \
    .option("header", True) \
    .option("multiline", True) \
    .option("sep", ";") \
    .option("quote", ";") \
    .option("dateFormat", "MM/dd/yyyy") \  # подсказка как парсить дату
    .option("inferSchema", True) \
    .load("./books.csv")

df.printSchema()
root 
 |-- id: integer (nullable = true)
 |-- authorId: integer (nullable = true)
 |-- releaseDate: date (nullable = true)
 ...
```

Логический вывод схемы (`inferSchema`) является мощной функциональной возможностью, но когда известная структура (схема) CSV-файла, может оказаться удобным явно определить типы данных, сообщив Spark рекомендуемую схему. _Логический вывод схемы_ -- ==дорогостоящая операция==, а ее явное определение позволяет более явно управлять типами данных. 

```python
from pyspark.sql.types import *

schema = StructType([
	StructField("id", IntegerType(), False),
	StructField("authorId", IntegerType(), True),
	StructField("title", StringType(), True),
	StructField("releaseDate", DateType(), True),
	StructField("link", StringType(), True),
])

# Обязательно нужно укзаать "dateFormat", иначе Spark не сможет распарсить дату
df = spark.read.format("csv") \
    .schema(schema) \  # <= NB
    .option("dateFormat", "MM/dd/yyyy") \  # <= NB
    .option("header", True) \
    .option("multiline", True) \
    .option("sep", ";") \
    .option("quote", "*") \
    .load("./books.csv")
```

### Чтение JSON-файлов

Прочитать JSON-файл можно аналогичным образом.
```python
df = spark.read.format("json") \
    .option("multiline", True) \  # <= NB
    .load("./countrytravelinfo.json")
```

Опцию `.option("multiline", True)` в данном случае нужно указывать обязательно!!! В противном случае JSON-файл будет прочитан неверно, будет состоять из одного столбца с именем `_corrupt_record`, а при попытке вывести содержимое кадра данных с помощью `df.show()` поднимется исключение `AnalysisException`.

### Чтение XML-файлов

Поскольку парсер не является частью стандартного дистрибутива Spark, необходимо отдельно скачать JAR-файл со страницы Maven https://mavenlibs.com/jar/file/com.databricks/spark-xml_2.12  (например, `spark-xml_2.12-0.16.0.jar`). 

Теперь при создании экземпляра Spark-сессии следует параметру `spark.jars.packages` передать значение `com.databricks:spark-xml_2.XX:0.XX.X` (сообразно названию скаченного JAR-файл)
```python
# read_xml.py
...

spark = SparkSession \
    .builder \
    .appName("XML Parser") \
    .config(
        "spark.jars.packages",
        "com.databricks:spark-xml_2.12:0.16.0"
    ) \
    .master("local[*]") \
    .getOrCreate()

df = spark.read.format("xml") \
    .options(rowTag="catalog") \
    .load(PATH_TO_XML_FILE)

df.printSchema()
```

А при запуске следует передать путь до скаченного JAR-файла
```bash
$ spark-submit --jars ~/spark/spark-xml_2.12-0.16.0.jar read_xml.py
```

### Проблемы с обычными форматами файлов

Обычные форматы файлов, такие как CSV, JSON, XML плохо подходят для работы с большими объемами данных по следующим причинам:
- данные в форматах JSON и XML (и в некоторой степени CSV) сложно сгементироватть (то разделить на сегменты для независимой обработки на различных рабочих узлах). Из-за наличия корневого элемента в формате XML потребуется изменение структуры, которое может нарушить корректность документа. Файлы больших данных должны быть легко разделяемыми.
- в формате CSV невозможно хранить иерархическую информацию.
- все обычные форматы весьма избыточны (особенно JSON и XML), что непосредственно влияет на размер файла.

Сводка по форматам для больших данных:
- Avro, ORC и Parquet -- широко распространенные форматы файлов больших данных. Они позволяют хранить данные и схему в файле и поддерживают _сжатие данных_.
- Файлы ORC и Parquet хранят данные в формате на основе столбцов, а формат Avro основан на строках, что больше подходит для потокового режима работы.
- Сжатие данных в форматах ORC и Parquet более эффективно, чем в формате Avro.
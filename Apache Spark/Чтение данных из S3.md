https://pythonhint.com/post/6809507411202675/read-files-from-s3-pyspark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ReadFromS3") \
    .config("spark.hadoop.fs.s3a.access.key", "ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

df = spark.read.csv("s3a://BUCKET_NAME/PATH/TO/FILE.csv", header=True)
```

Или так, вычитывая значения переменных окружения с помощью `dotenv`
```python
from dotenv import load_dotenv

load_dotenv()

conf = SparkConf()
conf.set('spark.hadoop.fs.s3a.access.key', os.environ['aws_access_key_id']) 
conf.set('spark.hadoop.fs.s3a.secret.key', os.environ['aws_secret_access_key'])
conf.set(
	'spark.hadoop.fs.s3a.aws.credentials.provider',
	'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'
)
spark = SparkSession.builder.config(conf=conf).getOrCreate()

multiline_df = spark.read.option("multiline","true") \
    .json("s3a://hand-on-from-s3-to-local-pyspark/example2.json")
```
Рекомендации по работе с S3:
- Следует использовать протокол `s3a`, а не `s3n` и `s3`, так как он работает быстрее.
- Нужно свести к минимуму количество маленьких файлов на S3. Маленькие файлы могут сделать S3 медленным и неэффективным. Рекомендуется собрать несколько маленьких файлов в один большой с помощью Hadoop FileManager или Parquet.
- Партиционирование следует использовать с осторожностью, так как с одной стороны оно может ускорить выполнение запросов, но с другой приведет к созданию большого числа маленьких файлов в S3. Нужно сбалансировать производительность запросов и эффективность S3.
- Для хранения данных следует использовать объектное хранилище, а не HDFS. 

Если нужно получить список файлов, хранящихся на S3, то можно поступить так
```python
# pip install s3fs
import s3fs

fs = s3fs.S3FileSystem(anon=True)
fs.ls("my-bucket")
```

Или так https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.hadoopFile.html
```python
spark.sparkContext.hadoopFile("s3a://bucket_name/prefix")
```

Пример связки Spark Structured Streaming + Kafka + MinIO можно найти здесь
https://github.com/minio/openlake/blob/main/spark/spark-streaming.md?ref=blog.min.io#pyspark-structured-streaming-application
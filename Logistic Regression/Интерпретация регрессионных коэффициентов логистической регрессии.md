Модель логистической регрессии, выраженная через _логит_^[Логит -- это натуральный логарифм от шанса]
$$
\begin{align*}
\ln \Big( \dfrac{p_i}{1 - p_i} \Big) = b_0 + b_1 x_i^{(1)} + b_2 x_i^{(2)} + \ldots,
\end{align*}
$$
где $p_i$ -- вероятность; $\dfrac{p_i}{1 - p_i}$ -- шанс; $\ln \big( \dfrac{p_i}{1 - p_i} \big)$ -- логит; $b_0$ -- константа; $b_1, b_2, \ldots$ -- регрессионные коэффициенты; $x_i^{(j)}$ -- $j$-ый признак $i$-ого наблюдения.

На практике модель логистической регрессии часто выражают через вероятность
$$
\begin{align*}
p_i = \dfrac{1}{1 + \exp^{-(b_0 + b_1 x_i^{(1)} + x_i^{(2)} + \ldots + b_k x_i^{(k)})}}
\end{align*}
$$

В логистической регрессии , в отличие от деревьев решений, спрогнозированных вероятностей 0 и 1 не существует. Это обусловлено свойствами логита. Отношение шансов исключают верхнюю границу вероятности, а логарифм шансов исключает нижнюю границу вероятности. В логистической регрессии для оценки параметров используется метод максимального правдоподобия. Цель метода максимального правдоподобия -- поиск параметров, максимизирующих правдоподобие наблюдаемых данных.

Коэффициент $b_i$ логистической регрессии (параметр модели, другими словами) показывает насколько _в среднем_ изменится _натуральный логарифм шанса события_ при изменении предиктора на единицу своего измерения, притом что остальные предикторы фиксированы.

Если коэффициент $b_i$ _положительный_, то с увеличением значения предиктора на единицу измерения натуральный логарифм шанса события возрастает. Если коэффициент $b_i$ _отрицательный_, то с увеличением значения предиктора на единицу своего измерения натуральный логарифм шанса события убывает [[Литература#^86c27e]]<c. 493>.

Пример. Пусть регрессионный коэффициент при предикторе _Стаж работы_ отрицателен и равен -0,243. Это означает, что при увеличении предиктора на единицу своего измерения, то есть при увеличении стажа работы на 1 год (притом что остальные предикторы остаются неизменными) _натуральный логарифм шанса_  $\ln \dfrac{p_i}{1 - p_i}$ просрочки 90+ уменьшается на 0,243.

Высокие значения $p$-value говорят о том, что регрессионный коэффициент не является статистически значимым, а _низкие значения $p$-value_, наоборот, свидетельствуют о _статистической значимости регрессионного коэффициента_ [[Литература#^86c27e]]<c. 494>

Перед построением модели логистической регрессии вы должны убедиться, что выполяются некоторые предпосылки:
- нормальное распределение,
- единый масштаб измерения переменных,
- отсутствие взаимосвязи между предикторами.

В отличие от линейной регрессии, логистическая регрессия не полагается на предположение о нормальном распределении. Однако ваше решение может быть _более стабильным_, если распределение предикторов будет _многомерным нормальным_.

Значительно различающиеся масштабы переменных затрудняют сравнение силы влияния предикторов, основываясь на величине коэффициентов, и делают бессмысленным использование регуляризации [[Литература#^86c27e]]<c. 529>

Третьей предпосылкой построения логистической регрессии является отсутствие взаимосвязи между предикторами. Наличие _сильной корреляционной взаимосвязи_ между предикторами называется _мультиколлинеарностью_.

Вспомним, что _регрессионный коэффициент показывает изменение натурального логарифма шанса события $\ln\dfrac{p_i}{1 - p_i}$ при изменении предиктора на единицу своего измерения, притом что все остальные предикторы фиксированы_. Последнее дополнение очень важно, ведь если интересующий нас предиктор будет сильно скоррелирован с другим, то при его изменении на единицу измерения будет меняться и другой предиктор, то есть _будет привнесено влияние другого предиктора_.

Мультриколлинеарность в модели будет проявляться в следующем [[Литература#^86c27e]]<c. 534>:
- сильный разброс оценок коэффициентов регрессии (большие положительные и большие отрицательные оценки коэффициентов регрессии, значительно выше 1,0 по модулю),
- резкое изменение оценок коэффициентов регрессии при добавлении или удалении предиктора,
- неправильный знак перед коэффициентом регрессии,
- присутствие в модели большого количества статистически незначимых оценок коэффициентов регрессии

Самый эффективный способ борьбы с мультиколлинеарностью --увеличение размера выборки. Увеличивая размер выборки, мы уменьшаем дисперсию регрессионных коэффициентов (параметров модели) и увеличиваем их статистическую значимость (стандартные отклонения регрессионных коэффициентов обратно пропорциональны $\sqrt{n}$, где $n$ -- размер выборки). Вообще говоря, проблема мультиколлинеарности характерна для небольших выборок и во многом снимается при работе с выборками достаточного объема [[Литература#^86c27e]]<c. 534>. Однако, следует иметь в виду, что ранее незначимые корреляции между предикторами с ростом размера выборки тоже могут становиться значимыми.

Второй способ борьбы с мультиколлинеарностью заключается в том, чтоб объединить высокоскоррелированные переменные в одну составную переменную (_агрегат_). Например, между двумя переменными _Сумма кредитных обязательств_ и _Доход_ может наблюдаться сильная корреляция, но при этом они несут ценность для модели и удалить их нельзя. Можно сформировать новую переменную, сформулировав ее как _Сумма кредитных обязательств_ / _Доход_ * 100%. 

Третий способ -- преобразование переменных с помощью метода главных компонент. Метод главных компонент преобразует набор коррелированных исходных переменных в набор некоррелированных главных компонент.

Четвертый и наиболее часто используемый способ -- это снижение разброса оценок коэффициентов регрессии с помощью методов регуляризации ($L_1$-, $L_2$-регуляризация).

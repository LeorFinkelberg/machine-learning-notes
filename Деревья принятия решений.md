Ремарка: деревья строятся жадно, с максимизацией информационного прироста (в случае задач классификации) на каждом расщиплении [[Литература#^ef7d57]]<p. 179>. Когда требуется построить прогноз, мы проходим по всему дереву. Деревья принятия решений ID3, C4.5 и C5.0 строятся с использованием связки "энтропия Шеннона и информационный прирост".

Энтропия Шеннона изменяется в диапазоне от 0 до 1. При 0 узел чистый, нет никакой энтропии, а при 1 -- узел максимально неопределенный. Выбирается то расщипление (признак и его пороговое значение), которое максимизирует информационный прирост, то есть разность между энтропией в родительском узле и взвешенной по поддержке суммы энтропий дочерних узлов.  

Есть и другой способ построения деревьев принятия решений -- на базе загрязненности Джини (Gini impurity)
$$
Gini = 1 - \sum p_k^2.
$$
Загрязненность Джини изменяется в диапазоне от 0 до 1. При 0 расщипление получается чистым (без загрязненности). При 0.5 расщипление случайное и полностью загрязненное. Оба класса представленны группами равной мощности. Значения больше 0.5 могут появляться, когда один из дочерних узлов не содержит ни одного наблюдения.
Деревья, построенные с помощью загрязненности Джини называются CART-деревьями (Classification and Regression Trees).

В задачах регрессии деревья также строятся жадно, но расщипление выбирается таким образом, чтобы разброс значений в каждом дочернем узле был как можно меньше.

Для небольших наборов данных каждое уникальное значение вещественного (непрерывного) признака используется в качестве порога расщипления. В случае больших наборов данных вещественные признаки разбиваются на определенное число квантилей. Это число можно задать в `maxBins` дерева принятия решения. После создания квантилей, среднее значение каждого бина будет использвоаться в качестве точки расщипления. Процедура повторяется для каждого вещественного признака на каждом разбиении. 

Подрезкой деревьев (чтобы деревья не переобучались) можно управлять с помощью следующих параметров:
- `maxDepth` - глубина дерева (по умолчанию 5),
- `minInstancesPerNode` - минимальное число экземпляров в листе. К примеру, если задать этот параметр равным 5, а при расщиплении в одном из дочерних узлов окажется менее 5 наблюдений, то расщипление выполняться не будет [[Литература#^ef7d57]]<p. 184>. Это логично (параметры подрезки деревьев имеют более высокий приоритет, чем максимальный информационный прирост), так как в противном случае было бы невозможно управлять сложностью дерева.
- `minInfoGain` - минимальный информационный прирост. Если информационный прирост рассматриваемого расщипления ниже этого значения, то расщипление не выполняется.
- `L1/L2 Regularization`  - $L_1$- и $L_2$ -регуляризация.

Ключевые гиперпараметры, которые влияют на производительность случайного леса:
- `featureSubsetStrategy` -- стратегия вычисления мощности случайного подпространства (`auto`, `all`, `sqrt`, `log2`, `onethird`) ,
- `subsamplingRate` -- размер подвыборки обучающего набора данных (от 0 до 1).

NB! В задачах с _несбалансированными классами_ может быть очень полезна модель градиентного бустинга [[Литература#^ef7d57]]<p. 190>.

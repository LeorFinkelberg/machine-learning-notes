Перед применением алгоритмов кластеризации необходимо данные отмасштабировать (привести к одному диапазону).
### Агломеративная кластеризация

Иерархические алгоритмы кластеризации подразделяют на _агломеративные_ и _разделительные_.

В случае агломеративной кластеризации мы начинаем с  изолированных экземпляров, каждый из которых находится в своем собственном кластере. Затем эти кластеры постепенно сливаются в один. В итоге получается дендограмма, для которой можно задать порог гетерогенности и получить количество кластеров, отвечающее этой гетерогенности.

В случае разделительной кластеризации ситуация обратная. Мы начинаем с одного большого кластера, а затем постепенно разбиваем его на более мелкие. Процедура повторяется до тех пор, пока каждый экземпляр не окажется в своем кластере.

Экземпляры можно объединять с помощью одиночной связи, полной связи, средней связи и ward (минимизирует дисперсию кластера).

Однако, не смотря на то, что иерархическая кластеризация освобождает пользователя от необходимости задавать явным образом число кластеров, этот ==алгоритм имеет большую временную и пространственную сложность порядка== $O(n^2 \, \log n)$, где $n$ -- число наблюдений  [[Литература#^ef7d57]]<p. 266>.

### k-Means с бисекциями

k-Means с бисекциями -- это гибрид иерархической разделяющей кластеризации и неиерархической кластеризации. Сначала мы разбиваем весь набор данных на 2 кластера. А затем к полученным кластерам примеяем k-Means. Так как малые кластера уже существуют, для них можно вычислить положение центройдов. Затем вычисляется сумма квадратов расстояний от каждой точки кластера до своего центройда (Sum of squared distance, SSE). Чем выше SSE, тем выше гетерогенность кластера. На следующем шаге мы выбираем кластер с наибольшим SSE и снова разбиваем его на 2 кластера. Теперь мы смотрим уже на 3 кластера и для каждого из них вычисляем сумму квадратов расстояний от каждой точки до своего центройда, а затем выбираем для бисекции кластер с наибольшим значением SSE.

k-Means с бисекциями работает очень быстро и на выходе (в отличие от k-Means) получаются кластеры с близким количеством экземпляров.

### Коэффициент силуэта

С помощью _коэффициента силуэта_ (Silhouette coefficient) можно определить "правильное" число кластеров. Коэффициент силуэта принимает значения из диапазона от -1 до +1. И значения близкие к 1 отвечают чистому разбиению.  Значение близкое к -1 означает наихудшую важность точки данных. Значения близкие к 0 укзаывают на то, что кластеры перекрываются.
```python
from pyspark.ml.evaluation import ClusteringEvaluator

...
evaluator = ClusteringEvaluator()
shilhouette = evaluator.evaluate(predictions)
```

Пример выбора числа кластеров можно найти здесь https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py.

При выборе числа кластеров следует обращать внимание на размеры кластеров и максимальное значение коэффициента силуэта. Кластеры должны быть сопоставимых размеров, а максимальное значение коэффициента силуэта не должно быть ниже среднего значения по всем кластерам.

![[sphx_glr_plot_kmeans_silhouette_analysis_003.png]]

NB! На [текущий момент](https://github.com/SeldonIO/MLServer/issues/1022) MLServer не поддерживает ОС Windows. При запуске команды `mlserver run .` возникает ошибка `AttributeError`
```bash
...
 File "C:\Code\test\.venv\lib\site-packages\mlserver\parallel\worker.py", line 24, in <module> IGNORED_SIGNALS = [signal.SIGINT, signal.SIGTERM, signal.SIGQUIT] AttributeError: module 'signal' has no attribute 'SIGQUIT'
 ```

Если коротко, то MLServer -- это сервер логического вывода (Inference Server), ориентированный на упрощение процедуры "сервировки" модели (преобразования ML-модели в сервис с доступом по REST API или gRPC).

MLServer^[Python-библиотека с открытым исходным кодом, предназначенная для разработки готовых к эксплуатации асинхронных APIs моделей машинного обучения] ориентирован на то, чтобы упростить процедуру развертывания ML-моделей с доступом по REST и gRPC, полностью совместимый со спецификацией [KFServing's V2 Dataplane](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html#reference-apis-v2-protocol--page-root):
- Поддерживает мульти-модельное развертывание, позволяющее нескольким моделям работать в рамках одного экземпляра сервера логического вывода MLServer.
- Поддерживает _пакетную обработку_^[В документации используется термин "адаптивная пакетная обработка"] запросов "на лету". Подробности можно найти [здесь](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html)

В рабочей директории проекта кроме Python-сценариев размещаются еще и специальные конфигурационные файлы: `settings.json` (необязательный; используется для локальной разработки) и `model-settings.json` для конфигурации модели.

Установить MLServer можно с помощью менеджера пакетов `pip`
```bash
$ pip install mlserver
```

Однако, если требуется какая-то дополнительная среда выполнения, например, `scikit-learn`, то нужно будет установить соответствующий пакет. В данном примере `pip install mlserver-sklearn`.

По умолчанию, MLServer создает пул, в котором для выполнения инференса используется только один рабочий процесс. Чтобы обойти ограничения GIL, MLServer создает пул воркеров, каждый из которых представляет собой отдельный процесс Python (со своей собственной GIL). Управление межпроцессным взаимодействием между главным процессом MLServer и пулом вокеров вывода приводит к некоторым накладными расходам. Под капотом MLServer использует библиотеку `multiprocessing`, которая реализует распределенное управление процессами, которая обеспечивает минимально возможные накладные расходы при реализации стратегий такого типа. Не смотря на то, что эти издержки сведены к минимуму, в некоторых случаях они могут быть заметны. Поэтому у пользователя есть возможность отключить параллельный вывод. Поле `parallel_workers` в файле `settings.json` (или альтернативно глобальная переменная окружения `MLSERVER_PARALLEL_WORKERS`) управляет размером пула воркеров:
- `N`, где `N > 0` создает пул воркеров размера `N`,
- `0`, отключает параллельный инференс. Другими словами, инференс будет выполняться в рамках главного процесса MLServer.

_Среды вывода_ (Inference runtimes) позволяют определить как ML-модель будет использоваться внутри MLServer. Об этом можно думать как о клее (backend glue), который связывает MLServer и вашу ML-модель.

MLServer "из коробки" поддерживает различные среды выполнения, но также позволяет создавать и [свои собственные](https://mlserver.readthedocs.io/en/latest/runtimes/custom.html). 

NB! При пытке отправить POST-запрос на запущенный MLServer может оказаться, что какой-то из портов, выделенных под MLServer (`8080`: HTTP-сервер, `8081`: gRPC, `8082`: сервер метрик) окажется занятым, скажем, MLflow. Освободить порт на MacOS можно так
```bash
$ netstat -vanp tcp | grep 8080  # предпологается, что занят порт 8080
$ lsof -i :8080  # выведет список процессов
$ kill -9 <PID>
```
